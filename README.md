
# PubMedCLIP
By far (2023/03/24) the sota method for `VQA-Rad` dataset.

[Arxiv paper: Does CLIP Benefit Visual Question Answering in the
Medical Domain as Much as it Does in the General Domain?](https://arxiv.org/pdf/2112.13906.pdf)

Dataset for VQA-Rad, see [Awenbocc/med-vqa](https://github.com/Awenbocc/med-vqa) (by checking issue QQ).
## 1. â›”ï¸ Try fine-tuning with `roco`:

Can't find the `/train/radiologytraindata.csv` in ROCO dataset repo.


## 2. âœ… Try training the data on `VQA-RAD`


According to the original paper,
> Our goal is to investigate the effect of using PubMedCLIP as a pre-trained visual encoder in MedVQA models. VQA in
this work is considered as a classification problem, where the objective is to find a mapping function f that maps an
imageâ€“question pair ($vi$, $qi$) to the natural language answer $ai$.
It looks like there are 2 pipelines for constructing the model pipeline:
    (a) MEVF: Overcoming Data Limitation in Medical Visual Question Answering
    (b) QCR: Medical Visual Question Answering via Conditional Reasoning [paper](https://dl.acm.org/doi/abs/10.1145/3394171.3413761?casa_token=E_IrwKfXPEMAAAAA:IC1Epmj0HbdWYzZWUfPpjbBJuMuL-iTdGbe1kVr5UQ4iVvfTgN_mgDBBEjyhqNBzRanKKlzyVQ)

**QCR (PubMedClip-RN50+AE)** acheives the  best accuracy.
training æŒ‡ä»¤ï¼š
`python main.py --cfg configs/qcr_pubmedclipRN50_ae_rad_16batchsize_withtfidf_nondeterministic.yaml`

### Bugfixes
- æˆ‘ä¸ç¢ºå®šå“ªè£¡å¯ä»¥ä¸‹è¼‰ `imgid2idx.json` çš„æª”æ¡ˆã€‚å·²ç¶“ä¿®å¥½ï¼ˆ`run.sh` å…§å¿˜è¨˜åŸ·è¡Œï¼‰ã€‚
```
# error message that does not seem to interfere: lib/utils/run.sh
Num of answers that appear >= 0 times: 557
Num of open answers that appear >= 0 times: 515
Num of close answers that appear >= 0 times: 72
found /home/nanaeilish/projects/mis/PubMedCLIP/QCR_PubMedCLIP/data/data_rad/cache/trainval_ans2label.pkl
Traceback (most recent call last):
  File "create_label.py", line 329, in <module>
    compute_target(train_qa_pairs, total_ans2label, intersection, 'train', img_col, data) #dump train target to .pkl {question,image_name,labels,scores}
NameError: name 'intersection' is not defined
```
- ä½¿ç”¨ `lib/utils/run.sh` å¾Œç”¢å‡ºçš„ä¸­é–“ç”¢ç‰©ï¼Œæœƒå°è‡´type_classifier checkpoint loading éŒ¯èª¤ã€‚
[issue with VQA-RAD training](https://github.com/sarahESL/PubMedCLIP/issues/9)
å› æ­¤æ ¹æ“šä½œè€…å»ºè­°å°‡ Awenbocc/med-vqa çš„ `data` è³‡æ–™å¤¾è¤‡è£½åˆ° `QCR_PubMedCLIP/data/data_rad` ä¸‹ã€‚

- `clip` çš„ä¸‹è¼‰è¦æŒ‰ç…§ï¼š[openai/clip](https://github.com/openai/CLIP#usage)ã€‚
- ç•™æ„ cuda driver ç‰ˆæœ¬å•é¡Œã€‚
- `QCR_PubMedCLIP/lib/utils` è³‡æ–™å¤¾ä¸‹çš„ `run.sh` è£¡é¢åªè¦åŸ·è¡Œ create_resized_images çš„ scripts å°±å¥½ï¼Œå…¶ä»–çš„æª”æ¡ˆç”¨ `Awenbocc/med-vqa` çš„ã€‚
- å¯ä»¥ç”¨é€™å€‹æŒ‡ä»¤ç¢ºèª $PWD åº•ä¸‹çš„.jpg images æ•¸é‡ï¼š`find . -name "*.jpg" | wc -l`

## âœ… 2.1 Training log & Tensorboards (2023/03/25)

Dataset for VQA-Rad, see [Awenbocc/med-vqa](https://github.com/Awenbocc/med-vqa) (by checking issue QQ).
```
2023-03-25 15:41:21,394 INFO     [Validate] Val_Acc:67.184036%  |  Open_ACC:46.408840%   |  Close_ACC:81.111107%
2023-03-25 15:41:22,982 INFO     [Result] The best acc is 67.184036% at epoch 22

2023-03-25 18:58:54,497 INFO     -------[Epoch]:199-------
2023-03-25 18:58:54,497 INFO     [Train] Loss:0.000347 , Train_Acc:99.184074%
2023-03-25 18:58:54,497 INFO     [Train] Loss_Open:0.000015 , Loss_Close:0.000026%
2023-03-25 18:58:56,648 INFO     [Validate] Val_Acc:69.844788%  |  Open_ACC:55.801105%   |  Close_ACC:79.259254%
2023-03-25 18:58:56,649 INFO     [Result] The best acc is 71.618622% at epoch 62 # paper: 72.1%
```
Train Acc å¤§æ¦‚åœ¨ 25 epochs æ™‚å°±å¹¾ä¹é”åˆ°æœ€é«˜å³°ã€‚
Closed çš„ VAcc å¾ˆä¸ç©©å®šï¼Œæœ‰å€‹ 2 ~ 3 % ä¹‹é–“çš„åŠ‡çƒˆä¸‹è·Œã€‚
Open çš„ VAcc æ•´é«”å‘ˆç¾ä¸Šå‡è¶¨å‹¢ã€‚
æ•´é«”è€Œè¨€å’Œ Paper çš„æ•¸æ“šæœ‰ç´„è« 0.5% çš„å·®è·ï¼ˆè¼ƒä½ï¼‰ã€‚

![](QCR_PubMedCLIP/output/qcr/pubmedclipRN50_ae/roco/VQARAD/QCR.CLIPRN50.AE.ROCO.VQARAD.16batchsize.200epoch.withTFIDF.nondeterministic/imgs/2023_0405_è¨“ç·´çµæœtensorboard.png)

## âœ… 2.2 Saving Pretrained checkpoints
saved at `/home/nanaeilish/projects/mis/PubMedCLIP/QCR_PubMedCLIP/output/qcr/pubmedclipRN50_ae`
## âœ… 3. Exporting the prediction on `VQA-RAD`

testing æŒ‡ä»¤ï¼š
`python main.py --test True --cfg configs/qcr_pubmedclipRN50_ae_rad_16batchsize_withtfidf_nondeterministic.yaml`

Add these (or at least your configured paths) into the config file `--cfg`.

```
MODEL_FILE: "PubMedCLIP/QCR_PubMedCLIP/output/qcr/pubmedclipRN50_ae/roco/VQARAD/QCR.CLIPRN50.AE.ROCO.VQARAD.16batchsize.200epoch.withTFIDF.nondeterministic/62_best.pth"
  RESULT_DIR: "PubMedCLIP/QCR_PubMedCLIP/output/qcr/pubmedclipRN50_ae/roco/VQARAD/QCR.CLIPRN50.AE.ROCO.VQARAD.16batchsize.200epoch.withTFIDF.nondeterministic/results"
```
### Issue
åœ¨å¯«æˆ testfile æ™‚ä»–çš„ Predicted_answer éƒ½åªå¯«å‡º closed_logits
æ˜¯ tensor çš„å½¢å¼ï¼Œæ²’æœ‰è½‰æ›æˆæ–‡å­—ï¼Œè¦è‡ªå·± decodeã€‚
[Hackmd Notes: How to get the inference data](https://hackmd.io/@NanaEilish727/pmclip)

### æˆç¸¾
`PubMedCLIP/QCR_PubMedCLIP/output/qcr/pubmedclipRN50_ae/roco/VQARAD/QCR.CLIPRN50.AE.ROCO.VQARAD.16batchsize.200epoch.withTFIDF.nondeterministic/medVQA.log` ä¸­çš„ validation best acc is `71.618622%` at epoch 62ï¼Œæœ€å¾Œ test ä½¿ç”¨ epoch 62 çš„checkpointçš„æˆç¸¾æ˜¯ `71.175163%`ã€‚æ ¹æ“š `/QCR_PubMedCLIP/main.py`ï¼Œtest mode ä¸‹ä¾ç„¶ä½¿ç”¨ val_loaderï¼Œå› æ­¤ä¸ç¢ºå®šç‚ºä½•æœƒæœ‰åˆ†æ•¸å·®ã€‚
```
451 179.0 272.0   # å…¨éƒ¨ï¼ŒOpenï¼ŒClose çš„æ•¸é‡
[Validate] Val_Acc:71.175163%  |  Open_ACC:56.424580%   |  Close_ACC:80.882355%
```

## ğŸ”¨4. Use the prediction and the data itself for data EDA

1. Data Exampleã€‚`question_type` å’Œ `phrase_type` çš„æ•¸é‡ã€‚
2. `question_type` å’Œ `phrase_type` level çš„ EDAï¼ˆé¡åˆ¥æ­£ç¢ºèˆ‡éŒ¯èª¤ç‡ï¼‰ã€‚


## Todo:
0. å®Œæ•´çš„è¨­å®šæª”ç†è§£ï¼ˆdata preproc åˆ°åº•åšåˆ°å“ªï¼Œæœ‰ resize å—ï¼Œæœ‰æ—‹è½‰å—ï¼‰
1. xai å¥—ä»¶å¦‚ LIME
2. medical preprocessing ï¼ˆå‚…ç«‹è‘‰è½‰æ›ã€è¼ªå»“æ¸…æ™° filterï¼‰å·¥å…·åŒ…è©¦ç”¨

